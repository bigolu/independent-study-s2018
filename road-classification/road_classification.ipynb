{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Road Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another project that we faced was classifying the roads of Shenzhen given training data in the form of CSV files. The data we were given was private vehicle trips and truck trips with the following facets: coordinates of the trip and timestamps of the trip. From this data we could formulize feature vectors that contained the following data about a particular road: median speed, average speed, 1/4 speed, 3/4 speed, whether there has been a truck/private vehicle on the road, ratio of trucks to private vehicles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools\n",
    "The tools that we chose to accomplish this goal are as follows:\n",
    "- From the Python stlib we used\n",
    "  - csv - to read the data\n",
    "  - statistics - for handy stat functions like mean, median, etc....\n",
    "  - IntEnum - We defined each row of our data in an int enum to make data access as readable as possible\n",
    "- Third party\n",
    "  - sklean - for machine learning algorithms\n",
    "  - numpy - for easy data formatting i.e. ensuring contiguous arrays\n",
    "  - dateutil - to reliably parse dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "An SVC for road types.\n",
    "\"\"\"\n",
    "import csv\n",
    "import collections\n",
    "import statistics\n",
    "from enum import IntEnum\n",
    "import math\n",
    "\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "from dateutil import parser\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enum\n",
    "What we learned from our previous work is that when reading code that access rows of data stored in lists, it can be very hard to follow if we simply use int indices. For this reason, we created an IntEnum to make our data accessing more readable as data\\[Column.DATE\\] probably gives more information than data[0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Column(IntEnum):\n",
    "    DATE = 0\n",
    "    TRIP_CHAR = 1\n",
    "    ROAD_ID = 2\n",
    "    LON = 3\n",
    "    LAT = 4\n",
    "    VEHICLE_TYPE = 5\n",
    "    LABEL = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables\n",
    "- CACHE_SIZE - this is the amount of memory that our SVM can use to process our data. We initially set this to 1000, but when we increased it, we found it was much faster, which makes sense.\n",
    "- label_names - this is a list of the names of each label, or street name. Since we mapped our labels to ints, this would allow us to access the actual name if we needed it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_SIZE = 4000\n",
    "label_names = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method was for getting the distance between to points in miles. We coul not find a library with an implementation of this so we opted to use this found on someones blog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distance between two points in miles\n",
    "# see: https://www.johndcook.com/blog/python_longitude_latitude/\n",
    "def distance_on_unit_sphere(lat1, long1, lat2, long2):\n",
    "    # Convert latitude and longitude to\n",
    "    # spherical coordinates in radians.\n",
    "    degrees_to_radians = math.pi/180.0\n",
    "\n",
    "    # phi = 90 - latitude\n",
    "    phi1 = (90.0 - lat1)*degrees_to_radians\n",
    "    phi2 = (90.0 - lat2)*degrees_to_radians\n",
    "\n",
    "    # theta = longitude\n",
    "    theta1 = long1*degrees_to_radians\n",
    "    theta2 = long2*degrees_to_radians\n",
    "\n",
    "    # Compute spherical distance from spherical coordinates.\n",
    "\n",
    "    # For two locations in spherical coordinates\n",
    "    # (1, theta, phi) and (1, theta', phi')\n",
    "    # cosine( arc length ) =\n",
    "    # sin phi sin phi' cos(theta-theta') + cos phi cos phi'\n",
    "    # distance = rho * arc length\n",
    "\n",
    "    cos = (math.sin(phi1)*math.sin(phi2)*math.cos(theta1 - theta2)\n",
    "           + math.cos(phi1)*math.cos(phi2))\n",
    "\n",
    "    # sometimes cos > 1?\n",
    "    if cos > 1:\n",
    "        # ¯\\_(ツ)_/¯\n",
    "        cos = 1\n",
    "\n",
    "    arc = math.acos(cos)\n",
    "\n",
    "    # Remember to multiply arc by the radius of the earth\n",
    "    # in your favorite set of units to get length.\n",
    "    return arc * 3960"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is for reading our input data from csv(s) into a list. We ran into many complications while doing so because of assumptions we made about the data. For example, We assumed that the data would begin with the start of a trip, but we found that it actually started in the middle of trip so we omitted that trip. We also found multiple 's' and 'e' chars together which did not make sense, so we removed the extras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(limit):\n",
    "    fnames = ['../data/pv/all.csv', '../data/truck/all.csv']\n",
    "    data = []\n",
    "\n",
    "    for fname in fnames:\n",
    "        with open(fname) as f:\n",
    "            vehicle_type = fname.split('/')[2]\n",
    "            seen_s = False\n",
    "            prev = None\n",
    "\n",
    "            reader = tqdm(csv.reader(f, delimiter=\",\"))\n",
    "            for count, datum in enumerate(reader):\n",
    "                if count >= limit:\n",
    "                    break\n",
    "\n",
    "                reader.set_description_str('Processing row {}/{} in {}'\n",
    "                                           .format(count, limit, fname))\n",
    "\n",
    "                datum = [parser.parse(datum[3]), datum[6].lower(),\n",
    "                         int(datum[7]), float(datum[8]), float(datum[9]),\n",
    "                         vehicle_type, datum[15]]\n",
    "\n",
    "                # filter out remainder of trip at beginning of data\n",
    "                # (data should start with an 's')\n",
    "                if datum[Column.TRIP_CHAR] == 's':\n",
    "                    seen_s = True\n",
    "                if not seen_s and datum[Column.TRIP_CHAR] != 's':\n",
    "                    continue\n",
    "\n",
    "                # filter out extra 'S' and 'E' chars\n",
    "                is_start_or_end = (datum[Column.TRIP_CHAR] == 's'\n",
    "                                   or datum[Column.TRIP_CHAR] == 'e')\n",
    "                same_char_as_prev = (prev\n",
    "                                     and (prev[Column.TRIP_CHAR]\n",
    "                                          == datum[Column.TRIP_CHAR]))\n",
    "                if is_start_or_end and same_char_as_prev:\n",
    "                    continue\n",
    "\n",
    "                data.append(datum)\n",
    "\n",
    "                prev = datum\n",
    "\n",
    "    # map labels to ints\n",
    "    global label_names\n",
    "    # get a set of all unique labels then convert to a list\n",
    "    # so that it is indexable\n",
    "    label_names = list(set([datum[Column.LABEL] for datum in data]))\n",
    "    for datum in data:\n",
    "        datum[Column.LABEL] = label_names.index(\n",
    "            datum[Column.LABEL])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    This is the function where we process the raw rows of data into feature vectors, one per road. Due to its complexity, we had to split it into two parts.\n",
    "    The first part was creating a dictionary of metadata for each road. This includes the number of trucks and private vehicles on the road, a list of all the speeds traveled on said road, and its classification. From this information, we could be the feature vectors.\n",
    "    Once this was complete, we just had to perform various stat functions on the speeds list and we would have our vectors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(data):\n",
    "    roads = collections.defaultdict(\n",
    "        lambda: {'num_truck': 0, 'num_pv': 0, 'speeds': [],\n",
    "                 'label': -1})\n",
    "\n",
    "    prev = None\n",
    "    data_tqdm = tqdm(data)\n",
    "    data_len = len(data)\n",
    "    for idx, datum in enumerate(data_tqdm):\n",
    "        data_tqdm.set_description_str('Processing datum {}/{}'\n",
    "                                      .format(idx, data_len))\n",
    "\n",
    "        date, trip_char, road_id, lon, lat, vehicle_type, label =\\\n",
    "            datum\n",
    "\n",
    "        roads[road_id]['label'] = label\n",
    "\n",
    "        # add to vehicle count\n",
    "        vehicle_key = 'num_truck' if vehicle_type == 'truck' else 'num_pv'\n",
    "        roads[road_id][vehicle_key] += 1\n",
    "\n",
    "        # add to speeds if not last point\n",
    "        part_of_same_trip = (prev\n",
    "                             and (prev[Column.TRIP_CHAR] == 's'\n",
    "                                  or prev[Column.TRIP_CHAR] == 'm')\n",
    "                             and (trip_char == 'm' or trip_char == 'e'))\n",
    "        if part_of_same_trip:\n",
    "            time_diff = date - prev[Column.DATE]\n",
    "            hours = time_diff.seconds / 60 / 60\n",
    "\n",
    "            prev_lat = prev[Column.LAT]\n",
    "            prev_lon = prev[Column.LON]\n",
    "            miles = distance_on_unit_sphere(lat, lon, prev_lat,\n",
    "                                            prev_lon)\n",
    "\n",
    "            # things happen\n",
    "            if miles == 0 or hours == 0:\n",
    "                continue\n",
    "\n",
    "            speed = miles / hours\n",
    "            roads[road_id]['speeds'].append(speed)\n",
    "\n",
    "        prev = datum\n",
    "\n",
    "    features = []\n",
    "    labels = []\n",
    "    roads_len = len(roads)\n",
    "    roads_tqdm = tqdm(enumerate(roads.items()))\n",
    "    for i, (k, v) in roads_tqdm:\n",
    "        roads_tqdm.set_description_str('Creating feature vector {}/{}'\n",
    "                                       .format(i, roads_len))\n",
    "\n",
    "        num_truck, num_pv, speeds, label = (v['num_truck'], v['num_pv'],\n",
    "                                            v['speeds'], v['label'])\n",
    "        speeds.sort()\n",
    "\n",
    "        # for some reason some roads have no speeds?\n",
    "        if not speeds:\n",
    "            continue\n",
    "\n",
    "        features.append([\n",
    "            # 1/4 speed\n",
    "            speeds[int(len(speeds) / 4)],\n",
    "            # median speed\n",
    "            statistics.median(speeds),\n",
    "            # 3/4 speed\n",
    "            speeds[int((len(speeds) / 4) * 3)],\n",
    "            # average speed\n",
    "            statistics.mean(speeds),\n",
    "            # existance of pv\n",
    "            1 if num_pv > 0 else 0,\n",
    "            # existance of truck\n",
    "            1 if num_truck > 0 else 0,\n",
    "            # percentage of pv on road\n",
    "            num_pv / (num_pv + num_truck),\n",
    "            # percentage of truck on road\n",
    "            num_truck / (num_pv + num_truck)])\n",
    "        labels.append(label)\n",
    "\n",
    "    return (features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method was for splitting our data into training and testing data. This was important because if we simply trained on the data and then tested on it, our score would be extremly biased, or overfitted, because it was trained on what is is being told to predict. We chose to ues 60% for testing and 40% for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_data(features, labels):\n",
    "    return [\n",
    "        np.ascontiguousarray(arr, dtype=np.float32)\n",
    "        for arr\n",
    "        in train_test_split(features, labels, test_size=.4, random_state=0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite being the least amount of code, this method actually takes up the majority of the runtime. That is because this is the method which trains the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classifier(features, labels):\n",
    "    print('Fitting model.....')\n",
    "    clf = svm.SVC(cache_size=CACHE_SIZE).fit(features, labels)\n",
    "    print('Fitting model DONE')\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lets us know how well we did. The best score we recieved was 30%. Despite how low this is, we are confident that with more training data, it could actually perform quite well. We only had 40,000 vectors, but I imagine we would need somthing in the millions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(clf, features, labels):\n",
    "    print('Calculating score.....')\n",
    "    score = clf.score(features, labels)\n",
    "    print('Calculating score DONE')\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runs the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    data = get_data(1_000_000_000)\n",
    "    features, labels = get_features(data)\n",
    "    f_train, f_test, l_train, l_test = get_train_test_data(features, labels)\n",
    "    clf = get_classifier(f_train, l_train)\n",
    "    score = get_score(clf, f_test, l_test)\n",
    "    print('SCORE: {}%'.format(int(score * 100)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
